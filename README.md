## Paper Summaries Repository

### Tags
`#approximation` `#theory` `#pruning` `#compression` `#quantization` `#neuralarchitecturesearch` `#nas` 

`#{YEAR}` `#activationfunction` `#lrscheduler` `#optimization` `#visualization` `#cnn` `#parameterreduction`

`#efficientmodels` `#lightweightmodels` `#modelenhancement` `#networkarchitecture` `#channelattention`

### Last Added

- [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size](./summaries/squeezenet/index.md)

  `#cnn` `#lightweightmodels` `#networkarchitecture` `#2016`

- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](./summaries/mobilenets/index.md)

  `#cnn` `#lightweightmodels` `#networkarchitecture` `#2017`
  
- [Squeeze-and-Excitation Networks](./summaries/squeeze-and-excitation-networks/index.md)

  `#cnn` `#modelenhancement` `#networkarchitecture` `#channelattention` `#2017`

- [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](./summaries/shufflenet/index.md)

  `#cnn` `#lightweightmodels` `#networkarchitecture` `#2017`

### All

- [ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices](./summaries/shufflenet/index.md)

  `#cnn` `#lightweightmodels` `#networkarchitecture` `#2017`

- [Squeeze-and-Excitation Networks](./summaries/squeeze-and-excitation-networks/index.md)

  `#cnn` `#modelenhancement` `#networkarchitecture` `#channelattention` `#2017`

- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](./summaries/mobilenets/index.md)

  `#cnn` `#lightweightmodels` `#networkarchitecture` `#2017`

- [SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size](./summaries/squeezenet/index.md)

  `#cnn` `#lightweightmodels` `#networkarchitecture` `#2016`

- [Mish: A Self Regularized Activation Function](./summaries/mish-activation/index.md)

  `#activationfunction` `#2019`

- [Visualizing the Loss Landscape of Neural Nets](./summaries/visualizing-the-loss-landscape-of-neural-nets/index.md)

  `#visualization` `#theory` `#losslandscape` `#2018`

- [Cyclical Learning Rates for Training Neural Networks](./summaries/cyclical-learning-rates-for-training-neural-networks/index.md)

  `#lrscheduler` `#optimization` `#2017`
  
- [SGDR: Stochastic Gradient Descent With Restarts](./summaries/stochastic-gradient-descent-with-restarts/index.md)

  `#lrscheduler` `#optimization` `#2016`
  
- [Searching for Activation Functions](./summaries/searching-for-activation-functions/index.md)

  `#activationfunction` `#neuralarchitecturesearch` `#nas` `#2017`

- [Gaussian Error Linear Units (GELUs)](./summaries/gaussian-error-linear-units/index.md)

  `#activationfunction` `#2016`

- [Learning Efficient Convolutional Networks through Network Slimming](./summaries/network-slimming/index.md)

  `#pruning` `#neuralarchitecturesearch` `#nas` `#2017`

- [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](./summaries/deep-compression/index.md)

  `#pruning` `#compression` `#quantization` `#2016`

- [Second order derivatives for network pruning: Optimal brain surgeon](./summaries/optimal-brain-surgeon/index.md)

  `#pruning` `#optimization` `#1992`

- [Optimal Brain Damage](./summaries/optimal-brain-damage/index.md)

  `#pruning` `#optimization` `#1989`

- [Approximation by Superpositions of a Sigmoidal Function](./summaries/approximation-by-superpositions-of-a-sigmoidal-function/index.md)

  `#approximation` `#theory` `#1989`